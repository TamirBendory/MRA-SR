\documentclass[english,12pt]{article}


\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\newcommand{\I}{\iota}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
%\newcommand{\tB}{\tilde{B}}
\newcommand{\tB}{B}
\newcommand{\hx}{\hat{x}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\SR}{super-resolution }
\usepackage[margin=3cm]{geometry}
\usepackage{color}
\newcommand{\TODO}[1]{{\color{red}{[#1]}}}


\newtheorem{thm}{Theorem}
\numberwithin{equation}{section}
\numberwithin{thm}{section} % important bit
\newtheorem{claim}[thm]{Claim}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}

\begin{document}

\title{The bispectrum can square your resolution: Super-resolution meets multi-reference alignment}

\author{Tamir Bendory}
\date{January 2019}
\maketitle


\section{Introduction}

We consider the problem of recovering a signal from multiple measurements, each is a noisy, rotated and sampled version of the signal. Let $x: S^1\rightarrow\R$ be a signal on the circle and  let $R_\theta$ denote a random rotation, that is, $(R_\theta x)(t)= x(t-\theta)$. 
We assume that $\theta$ is distributed uniformly on the circle and we let $P$ be the sampling operator that collects $L$ equally-spaced samples of the rotated signal. Together with i.i.d.\ Gaussian noise $\varepsilon\in \R^L$,  the model reads
\begin{align} \label{eq:model}
y &= P(R_\theta x) + \varepsilon, \quad & \theta\sim \text{Uniform}[0,1),\quad \varepsilon\sim \mathcal{N}(0,\sigma^2 I_L),
\end{align}
where $I_L$ is the identity matrix of size $L\times L$. The  samples of the $i$th measurement are then given by
\begin{equation} \label{eq:continuous_measurements}
y_i[\ell] = \left(R_{\theta_i} x\right)[\ell] + \varepsilon_i[\ell] =  x[\ell/L-\theta_i] + \varepsilon_i[\ell], \quad \ell=0,\ldots,L-1.
\end{equation}

Without any assumption on the structure of the signal, one cannot hope to accurately estimate it from finite number of samples. 
A natural and ubiquitous assumption is that the signal is bandlimited, that is, it can  be expanded as 
\begin{eqnarray} \label{eq:fourier_expansion}
x(t) = \sum_{b=-\tB}^{\tB}\hat{x}[b]e^{2\pi\I bt }, \quad t\in[0,1),
\end{eqnarray}
so that
\begin{eqnarray} \label{eq:fourier_expansion}
y_i[\ell] = \sum_{b=-\tB}^{\tB}\hat{x}[b]e^{2\pi\I b\left(\frac{\ell}{L}+\theta_i\right) }+ \varepsilon_i[\ell].
\end{eqnarray}
The largest non-zero frequency $\tB$ is called the bandlimit of the signal, and we denote $K:=B/L$.
We aim to recover the signal $x$ from $y$, up to global rotation. The rotations are referred to as \emph{nuisance variables}---while they are unknown,the goal of the task is not estimating them.

The classical sampling theory for a single measurement---the Shannon-Nyquist sampling theorem--- states that number of samples $L$ should be, at least, $2\tB+1$~\cite{shannon1949communication}. (For a recent comprehensive survey of modern sampling theory; see~\cite{eldar2015sampling}.)
In this work, we are interested in  the \emph{high-noise, few samples regime}  in which $\sigma$ is large  and $L\leq 2\tB$. 
The difficulty is then resides in several factors: high noise, unknown rotations and low sampling rate.
In particular, we study the interplay between four parameters: the bandlimit $\tB$, the number of samples $L$, the noise level $\sigma$, and the number of measurements $N$. %We denote the ratio $K=\tB/L$ as the \emph{super-resolution factor}.
Informally, the main theoretical result of this paper states the following:
\begin{thm}[informal]
Suppose that $N\to\infty$ measurements from the model~\eqref{eq:model} are collected in the high noise regime and each measurement is sampled at $L$ equally-spaced positions. 
Then, if $N$ scales at least like $\sigma^6$, one can identify up to $B\approx L^2/6$ frequencies. %That is, one can identify (order of) $L^2$ from $L$ samples per measurement. 
\end{thm}

The problem of estimating a high-resolution signal from low-resolution measurements (e.g., a few samples) is usually referred to as \emph{super-resolution from multiple measurements}. This problem attracted the attention of numerous researchers  in the last couple of decades in a variety of fields, such as computer vision, image processing and medical imaging~\cite{park2003super,farsiu2004advances, greenspan2008super}.
Since given the rotations the problem is linear, many papers have focused on the problem of estimating the set of $\theta_i$. However, in the high noise regime---which is the focal point of this work---the rotations cannot be estimated reliably by any method~\cite{bendory2018toward,aguerrebere2016fundamental}. 
We mention by passing that many works in recent years studied super-resolution from a single image based on prior knowledge (such as sparsity~\cite{huang2009super,candes2014towards} and recurrence of patches within the same image~\cite{glasner2009super}),
and learning~\cite{lim2017enhanced}; just to mention few works out of a huge literature on the subject. 

The model~\eqref{eq:model} is a special case of the \emph{multi-reference alignment} (MRA) problem. This problem involves estimating a signal (which lies in a known vector space) from multiple noisy measurements; in each measurement the signal is acted upon by an unknown element of a known group $G$. 
In its most general form, the measurement model can be written as 
\begin{equation} \label{eq:mra}
y = T(g\circ x) +\varepsilon, \qquad g\in G,
\end{equation}
where $T$ is a known linear operator. 
In our model~\eqref{eq:model}, the signal lies in the space of B-bandlimited signals, $G$ the rotation group $U(1)$ (isomorphic to the  two-dimensional rotation group $SO(2)$), and the linear operator is the point-wise sampling $P$.
Similarly to many MRA models in the literature,  our work is also inspired by single particle reconstruction problems using cryo-electron microscopy (cryo-EM) and x-ray free electron lasers (XFEL)---high-resolution structural methods for biological macromolecules~\cite{frank2006three,kuhlbrandt2014resolution,singer2018mathematics}. 
In particular, this work is a first step towards understanding the resolution limits in these modalities. In Section~\ref{sec:future_work} we make this connection explicit.

In the low noise regime, one can try to estimate the unknown group elements (that is, rotations) using one of many synchronization methods~\cite{singer2011angular,bandeira2015non,boumal2016nonconvex,chen2018projected,singer2011three}.
Once those elements are known, estimating $x$ is a linear, and relatively easy, problem. 
As aforementioned, one cannot estimate the missing rotations of~\eqref{eq:model}---by any method---in the high noise regime~\cite{bendory2018toward,aguerrebere2016fundamental}.
To circumvent this impossibility result, we study two methods.
The first approach relies on which are invariant 
under rotations. In the limit of $N,\sigma\to\infty$, it was shown that this strategy achieves the optimal  estimation rate---that is, no other method can obtain accurate estimation of the signal with fewer measurements~\cite{bandeira2017optimal,bandeira2017estimation,abbe2018multireference,abbe2018estimation}. Based on this approach, different algorithmic methods were proposed under different MRA setups~\cite{bendory2017bispectrum,perry2017sample,abbe2018multireference,boumal2018heterogeneous,chen2018spectral,ma2018heterogeneous,bandeira2014multireference}, as well as for  cryo-EM and XFEL~\cite{kam1980reconstruction,liu2013three,kurta2017correlations,levin20173d,bendory2018toward,pande2018ab,von2018structure}. 

From reasons that are explained later on, we employ the invariant approach only for the theoretical analysis, carried out in Sections~\ref{sec:background} and~\ref{sec:theory}. As a computational framework, we devise an expectation-maximization (EM) algorithm, which marginalizes over the rotations rather than estimating them explicitly.  The EM algorithm is explained in detail in Section~\ref{sec:EM}.
Numerical experiments show that this EM algorithm enables to resolve frequencies well beyond the Nyquist rate.

\section{Background} \label{sec:background}

Our theoretical analysis is based on considering the invariant features of a discrete model. In this section we first elaborate on the invariant approach. Then, we describe the discrete model and show that our theoretical results on this model apply directly to~\eqref{eq:model}.

\subsection{Rotationally invariant features} \label{sec:invariants}

The $q$th order rotationally invariant feature of a signal $z\in S^1$ is simply its autocorrelations:
\begin{equation}
M_q(z)[\ell_1,\ldots,\ell_{q-1}]=\int_{0}^1 z(t)z(t+\ell_1)\ldots z(t+\ell_{q-1})dt.
\end{equation} 
It is readily seen that this quantity remains unchanged under any rotation. 

The invariants  can be also understood in the Fourier domain.
Let ${\hat{z}[k]}$ be the $k$th Fourier coefficient of $z$. Recall that rotating the signal by an angle $\alpha$ in the space domain is equivalent to multiply ${\hat{z}[k]}$ by $e^{\I k\alpha}$. Hence, the monomials
\begin{equation}
\hat{M}_q(z)[k_1,\ldots,k_{q-1}]=\hat{z}[k_1],\ldots \hat{z}[k_{q-1}]{\hat{z}[-k_1-\cdots-k_{q-1}]},
\end{equation} 
are also invariant under rotation. If $z$ is a discrete signal, then integration is replaced by a sum.

In this work, we make use of the first three invariants. The first invariant is the zero frequency $\hat{M}_1(z) = \hat{z}[0]$ (equivalently, the mean of the signal). The second invariant is the power spectrum of the signal $\hat{M}_2(z)[k]=|z[k]|^2$. Unfortunately, the mean and the power spectrum do not determine a signal uniquely. 
Thus, we make use of the third invariant, the \emph{bispectrum}, which determines almost all signals uniquely~\cite{tukey1953spectral,sadler1992shift}. Explicitly, it is given by
\begin{equation}
\hat{M}_3(z)[k_1,k_2] = z[k_1]z[k_2]z[-k_1-k_2].
\end{equation}


The bispectrum is a key ingredient in many data processing applications, such as separating Gaussian and non-Gaussian processes~\cite{brockett1988bispectral}, studying the cosmic background
radiation, seismic, radar and EEG signals~\cite{wang2000cosmic,chen2008feature,ning1989bispectral}, MIMO systems~\cite{chen2001frequency}, and classification~\cite{zhao2014rotationally}.  
In Section~\ref{sec:future_work} we discuss the ramifications of employing higher-order invariants.  

One remarkable drawback of using high-order autocorrelations (or invariants) is their noise-amplification property. Computing the $q$th  autocorrelation involves the product $q$ noisy terms and thus the variance of the autocorrelation of a single measurement is proportional to $\sigma^{2q}$ in the high noise regime. Therefore, in this regime, the the number of measurements should scale like $\sigma^{2q}$ to retain a constant estimation error. While it seems to be a large number of measurements, it was shown that for the multi-reference alignment model~\eqref{eq:mra} the invariant approach (or, more generally, the method of moments) is optimal in the following sense. Let $\bar{q}$ be the lowest-order autocorrelation that distinguish two signals (up to symmetries). Then, in the $N,\sigma\to\infty$ regime, the error of any method is bounded away from zero if  $N/\sigma^{2\bar{q}}$ is bounded from above~\cite{bandeira2017optimal,abbe2018estimation}. 
In our setup~\eqref{eq:model}, since the power spectrum does not determine the signal (even without the sampling operator), the optimal number of of required measurements scales as $\sigma^6$. In Section~\ref{sec:future_work} we discuss the potential of super-resolution if more measurements are available.


\subsection{The discrete model} \label{sec:discrete_analysis}

In this section, we describe a discrete model. Then, we show that
any result on the discrete model based on autocorrelations hold true for~\eqref{eq:model}. To make the distinction clear, we refer here to~\eqref{eq:model} as the continuous model.

Let us denote by $\T_T\subset S^1$ the T-points equally-spaced grid 
 \begin{equation} \label{eq:grid}
 \mathcal{T}_T:=\left\{0,\ldots,\frac{T-1}{T}\right\}.
 \end{equation}
 In the discrete model, we study the case where we restrict $\theta$ to rotated only on the grid $\T_B$ (rather than continuously on $S^1$) and then sample on $\T_{B/K}=\T_{L}$.
 Under this model, the samples of the $i$th measurement are given by
 \begin{equation} \label{eq:discrete_model}
 y_i^d[\ell] = x\left(\frac{\ell K+\tau_i}{B}\right) + \varepsilon_i[\ell]=x\left(\frac{\ell}{L} + \frac{\tau_i}{B}\right)+\varepsilon_i[\ell], \quad \ell=0,\ldots L-1,
 \end{equation}
 for some random $\tau_i\in\{0,\ldots B-1\}$. The super script $y^d$ indicates that the measurements are taken from the discrete model. 

Note that the grid $\T_B$ samples $x$ below its Nyquist rate and thus induces aliasing in the Fourier domain.
The following lemma reveals the effect of this aliasing and formulates the connection between the autocorrelations in the discrete and the continuous models~\eqref{eq:model}. 
Before we present the lemma,  recall that 
by the law of large numbers 
\begin{equation}
\lim_{N\to\infty}\frac{1}{N}\sum_{i=1}^NM_q(y_i)[\ell_1,\ldots,\ell_{q-1}]
=\E M_q(y)[\ell_1,\ldots,\ell_{q-1}], 
\end{equation}
almost surely. 
Thus, to simplify notation, we take expectation over the invariants in the lemma. 
The proof of the lemma is technical and we leave it to Appendix~\ref{sec:proof_lem:discrete_continuous}.
\begin{lemma} \label{lem:discrete_continuous}
Let $y$ and $y^d$ denote the observations in~\eqref{eq:model} and~\eqref{eq:discrete_model}, respectively.
Then, 
\begin{align}
\E M_1(y) = \E M_1(y^d) &= \hat{x}[0],
\end{align}
and
\begin{align} \label{eq:power_spectrum_discrete}
\E M_2(y)[n] = \E M_2(y^d)[n] &= \sum_{b=-\tB}^{\tB}\vert \hat{x}[b]\vert ^2e^{2\pi\I bn/L} + \sigma^2\delta[n].
\end{align}
The third-order autocorrelation of the  discrete model is given by 
\begin{align} \label{eq:bispectrum_periodic}
\E M_3^d[n_1,n_2] &= \sum_{b_1=-\tB}^{\tB}\sum_{b_2=-\tB}^{\tB}\hat{x}[b_1]\hat{x}[b_2]\hat{x}[(-b_1-b_2)\bmod B]e^{-2\pi\I (b_1n_1 + b_2n_2)/L} + B_3[n_1,n2],
\end{align}
where $B_3[n_1,n2] = \sigma^2\hat{x}[0](\delta[n_1,0]+\delta[0,n_2] + \delta[n_1,n_2])$.
The third-order autocorrelation of the continuous model takes on the form 
\begin{align} \label{eq:bispectrum_aperiodic}
\E M_3^d[n_1,n_2] &= \sum_{b_1=-\tB}^{\tB}\sum_{b_2=-\tB}^{\tB}\hat{x}[b_1]\hat{x}[b_2]\hat{x}[(-b_1-b_2)]e^{-2\pi\I (b_1n_1 + b_2n_2)/L}+B_3[n_1,n2].
\end{align}
\end{lemma}

Lemma~\ref{lem:discrete_continuous} shows that the first two autocorrelations of the discrete and continuous setups are identical, while the third differs in the regime $|b_1+b_2|\geq\tB$.
It is therefore suggests a simple way to reduce~\eqref{eq:model} to the discrete setup. This can be done by extending each measurement periodically 
\begin{eqnarray} \label{eq:fourier_expansion_period}
\tilde{y}_i[\ell] = \sum_{b=-\infty}^{\infty}\hat{x}[b\bmod B]e^{2\pi\I b\left(\frac{\ell}{L}+\theta_i\right) }+ \tilde\varepsilon_i[\ell],
\end{eqnarray}
where $\tilde\varepsilon_i$ is the periodic extension of $\varepsilon_i$. \TODO{to recheck!}
By comparing~\eqref{eq:bispectrum_aperiodic} and~\eqref{eq:bispectrum_periodic} it is clear that the third-order moment of~\eqref{eq:fourier_expansion_period} equals to the third-order autocorrelation of the discrete case. Based on this reduction, we conclude the following:

\begin{cor} \label{cor:discrete_contonuous}
	Every result for the discrete model~\eqref{eq:discrete_model} which is based solely on the first three autocorrelation, holds for~\eqref{eq:model} as well.
\end{cor}


Corollary~\ref{cor:discrete_contonuous} implies that estimating from  periodic autocorrelations is at least as hard as estimating from the aperiodic counterparts.
This phenomenon is not unique to our setup, and arises in a variety of setups.  
For instance, it is well know that a generic two-dimensional signal can be determined uniquely from its aperiodic second-order autocorrelation but not from its periodic autocorrelation; see for example~\cite{hayes1982reconstruction}.\footnote{In practice, the mapping between the second-order aperiodic autocorrelation and a signal might be extremely sensitive~\cite{barnett2018geometry}.} The aperiodicity also plays a key role in uniqueness results other related fields, such as ultra-short pulse characterization~\cite{bendory2018signal}. 

This work can be also understood in the context of sampling theorem based on autocorrelations: how many samples are required if one can compute the autocorrelations of the signal?
 Indeed, the expressions~\eqref{eq:power_spectrum_discrete} and~\eqref{eq:bispectrum_periodic} represent sub-sampled versions of the full autocorrelations. In the Fourier domain, we get aliasing, that is, folding of the power spectrum and bispectrum onto themselves (for details, see Appendix~\ref{sec:proof_lem:discrete_continuous}):
\begin{align}
\frac{1}{L}\E \hat{M}_2(y^d)[k] &=  \sum_{p=0}^{K-1} \vert \hat{x}[(q+PL)\bmod L]\vert^2 \\
\E \hat{M_3}(y^d)[k_1,k_2] &=  
 \sum_{p_1,p_2=-(K-1)}^{K-1} \hat{x}[(k_1 + p_1L)] \hat{x}[(k_2 + p_2L)]\hat{x}[(-q_1-q_2 - (p_1+p_2)L)\bmod B]. \nonumber
\end{align}
In the next section, we show that one can recover a signal uniquely from these aliased quantities. 

\section{Main results}
\label{sec:theory}

To analyze this model, we introduce notation
\begin{equation} \label{eq:sub_signals}
x_i[\ell] = x[K\ell + i], \quad \ell=0,\ldots,L-1.
\end{equation}
Using~\eqref{eq:sub_signals}, one can think about each measurement in the discrete model 
as a two stages procedure. First, one of the $K$ signals~\eqref{eq:sub_signals} is chosen uniformly at random. Then, a random cyclic shift on the grid $\T_L$ is acted upon the chosen signal (of length $L$).
Therefore, the model can be reformulate  as 
\begin{equation} \label{eq:heter_mra}
y^d =  R_{\theta_{L}} x_{v} + \varepsilon,
\end{equation}
where $v$ distributed uniformly over $\{0,\ldots,K-1\}$ and $R_{\theta_L}$ is a circular shift on the grid $\T_L$. 

The model~\eqref{eq:heter_mra} has been studied in the literature under the name \emph{heterogeneous multi-reference alignment} and analyzed using the bispectrum~\cite{perry2017sample,bandeira2017estimation,boumal2018heterogeneous}. 

In particular, it was shown that in the limit of $N\to\infty$ the linear combination of the invariants can be easily estimated from the data by 
\begin{align} \label{eq:mix_invariants}
\frac{1}{N}\sum_{i=1}^N \mu_{y_i^d} &= \frac{1}{K}\sum_{i=1}^K \mu_{x_i}, \nonumber\\
\frac{1}{N}\sum_{i=1}^N  P_{y^d}[k] &= \frac{1}{K}\sum_{i=1}^K P_{x_i}[k] + \sigma^2L\mathbf{1}, \quad k=0,\ldots,L-1\\
\frac{1}{N}\sum_{i=1}^N B_{y^d}[k_1,k_2] &= \frac{1}{K}\sum_{i=1}^K (B_{x_i} [k_1,k_2]+\mu_{x_i}\sigma^2L^2 A[k_1,k_2] ), \quad  k_1,k_2=0,\ldots,L-1 \nonumber
\end{align}
where $\mathbf{1}\in\mathbb{R}^L$ is a vector of ones, and $A\in\mathbb{R}^{L\times L}$ is a zero matrix except $A[0,0]=3$ and $A[i,0]=A[0,i]=A[i,i]=1$ for $i=1,\ldots,L-1$.
Given $\sigma^2$, one can remove the bias factors easily. 

The identifiability of a set of $K$ signals from the mix of their  invariants~\eqref{eq:mix_invariants} was studied~\cite{bandeira2017estimation}.
Clearly, one can hope to identify the signals only up  to permutation (between the $K$ signals) and cyclic shifts (of each signal separately). 
Hence, at best, one can identify the set of signals  
\begin{equation} \label{eq:setS}
\mathcal{S}(x_1,\ldots,x_K):=\left\{ \theta_0,\ldots,\theta_{K-1},\pi : R_{\theta_0}x_{\pi(0)}, \ldots,R_{\theta_{K-1}}x_{\pi(K-1)},
\right\}
\end{equation}
for cyclic shifts $R_{\theta_i}$ for $\theta_i=0,\ldots,L-1$, and a permutation $\pi$.
In this work, it was shown that $K$ generic signals can be recovered from~\eqref{eq:mix_invariants},  as long as $K<\mathcal{P}(L)$, where
\begin{equation} \label{eq:Pl}
\mathcal{P}(L) := \frac{L+3+\left\lfloor L/2\right\rfloor +  \left\lceil (L-1)(L-2)/6\right\rceil}{L+1}.
\end{equation}
For $K\geq 5$, it suffices to require $K\leq \frac{L+5}{6}$. 
Since $\mathcal{P}(L) \approx L/6$, we conclude that the minimal number of samples scales as $\sqrt{6B}$. In other words, from $L$ samples, one can approximate  $B=O(L^2)$ frequencies. 
This bound was proved only in the regime $L\leq 192$, and therefore limiting our rigorous proof for the regime of small $L$\footnote{The proof of the result is based on numerical examination of a special matrix that should be carry out for each set of parameters. In~\cite{bandeira2017estimation} the conjecture was verified for a limited regime of parameters. Thanks to the assistance of Dr. Joseph Kileel we were able to verify the conjecture for broader set of parameters.}. 
However, it was conjectured  to hold true for any $L$~\cite[Conjecture 5.4]{bandeira2017estimation}.
Therefore, we conclude the following result:
\begin{prop} \label{prop1}
Consider the  discrete periodic model~\eqref{eq:discrete_model} when $N\to\infty$. 
For $L\leq 180$ and  $B/L<\mathcal{P}(L)$, one can identify the set of signals $\mathcal{S}(x_1,\ldots,x_K)$.
If Conjecture 5.4 in~\cite{bandeira2017estimation} is valid, the it holds true also for any $L$.
\end{prop}	

It is important to note that Proposition~\eqref{prop1} is theoretical in the sense that it is not clear whether the bound~\eqref{eq:Pl} can be achieved computationally.  
This question was studied empirically in~\cite{boumal2018heterogeneous}. Numerical evidences suggest that the maximal number of signals that can recovered, up cyclic shifts and permutation, from~\eqref{eq:mix_bispectra} is $\sqrt{L}$. 
Strong theoretical support was provided in~\cite{weinthesis}. In particular,  it was proven that $K$ signals whose entries i.i.d.\ Gaussians can be recovered with high probability from their mixed bispectrum as long as $k\leq \sqrt{L}/\text{polylog}(L)$.
In this case, the highest frequency that can be estimated scales as $B\leq L^{3/2}$ (up to possible log factors). 
\TODO{This is true for signals with white spectrum... for our case the bound seems smaller since the signals are smooth}

Proposition~\ref{prop1} suggests that all entries of the underlying signal can be identified in the regime of~$\mathcal{P}(L)L>B$. However, given all possible cyclic shifts and permutations, there are $(L^K)\cdot K!$ \TODO{check this number}  different ways to order the signals $x_1,\ldots,x_K$. Hence, to guarantee unique recovery of the signal, up to cyclic shift, one must impose some prior on the signal, such as smoothness. In the following proposition we show that such unique ordering is possible under rather mild conditions.
%The proposition is proved  in Appendix~\ref{sec:proof_bandlimit}.
\begin{prop} \label{prop:bandlimit}
Suppose that $x$ is generic, and let 
\begin{equation} %\label{eq:sub_signals}
x_i[\ell] = x[K\ell + i], \quad \ell=0,\ldots,L-1.
\end{equation}
In addition, suppose that $\hx[m]=x_m$ for some $m$ and $x_m$.
If $x_m\neq 0$, then there exists only one signal in $\mathcal{S}$ obeying $\hx[m]=x_m$. If $x_m=0$, then also the all the cyclic shifts of the signal satisfies the constraint (but only them). 
\end{prop}	
\begin{proof}
Suppose that $x_1,\ldots,x_K\in\R^L$ are known.
The combined signal is then given by 
\begin{equation}
x = \sum_{i=1}^K T_kZR_kx_k,
\end{equation}
where $R_k$ are (unknown) cyclic shifts of the signals $x_k$, $Z$ denotes a zero-padding operator (by a factor of $K$) so that $Zx_k\in\mathbb{R}^B$, and $T_k$ shifts the zero-padded signals by $k$ entries for $k=0,\ldots, K-1$. 
For instance, for $K=3$ we have: 
\begin{equation}
Zx = (x[0],0,0,x[1],0,0,x[2],...)\in\mathbb{R}^B.
\end{equation}

The Fourier transform of a signal $u\in\mathbb{R}^{L}$ is defined by 
\begin{equation}
\hat{u}[m]=\sum_{\ell=0}^{L-1}u[\ell]e^{-2\pi\I m\ell /L}, \quad m=0,\ldots,L-1. 
\end{equation}
The Fourier transform of $Zu$ is then given by 
\begin{equation}
\widehat{(Zu)}[m] = \sum_{\ell=0}^{L-1}u[\ell]e^{-2\pi\I m\ell /(B/K)}, \quad m=0,\ldots,B-1.
\end{equation}
In other words, $\widehat{(Zu)}[m] = \hat{u}[m\bmod L]$.

The Fourier transform of $ZR_{r_k}u$ is  given by 
\begin{equation}
\widehat{(ZR_{r_k}u)}[m] = e^{-2\pi\I m r_k L} \hat{u}[m\bmod L], \quad m=0,\ldots,B-1.
\end{equation}
This signal is shifted once again and we get
\begin{eqnarray} \label{eq:Fourier_structure}
\hat{x}[m] = \sum_{k=1}^K e^{-2\pi\I m( p_k+r_k K)/B} \hat{x}_k[m \bmod L]. 
\end{eqnarray}
for $m=0,\ldots,B-1$, 
where $r_k$ is a chosen randomly uniformly over $\{0,\ldots,L-1\}$, independently for each $k$, and $p_k$ represents the permutations, that is, $p_k\in\{0,\ldots K-1\}$, $p_k\neq p_\ell$ for all $k\neq \ell$ (note that we do have the freedom to fix one signal and its shift). 
\end{proof}
Our main interest is in bandlimited signals for which the spectrum vanishes starting from some frequency. Proposition~\ref{prop:bandlimit} implies that generically there exists at most one bandlimited signal in  $\mathcal{S}(x_1,\ldots,x_K)$.  
Hence, we conclude the following:
\begin{thm} \label{thm:discrete}
Consider the  discrete periodic model~\eqref{eq:discrete_model} when $N\to\infty$. Suppose that one entry of $\hx$ is known to be zero (e.g., $x$ is a bandlimited signal for which many of Fourier coefficients are zero). 
Then, For $K\leq 15$ and for $\mathcal{P}(L)L>B$ (given in~\eqref{eq:Pl}), one can identify $x$ uniquely, up to shift. 
If Conjecture 5.4 in~\cite{bandeira2017estimation} is valid, the it holds true also for any $K$.
\end{thm}

\TODO{Here we should explain an important consequence: one can sub-sample the autocorrelation}


%We should discuss mention that our result say that you can sub-sample the third-order autocorrelation by a large factor and still identify the signal. This is somehow related to the sub-Nyquist literature. 


\section{An expectation-maximization algorithm }
\label{sec:EM}

Our theoretical study is based on invariant features of order three. Conceptually, it suggests a two stages procedure: it starts with an estimating the set of all possible rotations and permutations $\mathcal{S}(x_1,\ldots,x_K)$~\eqref{eq:setS}, and then identifying one unique signal according to some prior (e.g., smoothness, bandlimit).
While efficient bispectrum demixing is possible~\cite{boumal2018heterogeneous,weinthesis}, it is not clear how to devise a tractable algorithm that finds a unique signal out from $\mathcal{S}(x_1,\ldots,x_K)$ that agrees with the prior.  %Second, any error in estimating the set $\mathcal{S}(x_1,\ldots,x_K)$ may lead to a . Hence, such a procedure may be very sensitive to errors. 
As an alternative, we devise an expectation-maximization (EM) algorithm described below that treats the estimation problem as a whole.

EM is a popular  framework to compute the  maximum likelihood estimator (MLE)~\cite{dempster1977maximum}. While it lacks rigorous theory, it is known to work quite well in many cases. 
Hereafter, we formulate the EM for the general model
\begin{equation}
y = T R_\theta x + \varepsilon, 
\end{equation}
where $R_\theta$ is restricted to rotate on a discrete grid $\mathcal{T}$ and $T$  is a general linear operator (not necessarily a sampling matrix).
In the end of this section, we make few comments on the particularization to the specific case where $T$ is the sampling operator.

Given a set of $N$ independent observations $y_1,\ldots,y_N$, the likelihood function  is proportional to $\mathcal{L}(y_1,\ldots,y_N) \propto	 p(y_1,\ldots,y_N|x)p(x)$, where  
\begin{equation} \label{eq:likelihood}
p(y_1,\ldots,y_N|x)\sim \prod_{i=1}^N\frac{1}{|\mathcal{T}|}\sum_{{\theta_\ell}\in\mathcal{T}}\mathcal{N}(T R_{\theta_{\ell_i}}x,\sigma^2),
\end{equation}
and $p(x)$ is a prior on the signal. 
We assume that the signal is drawn randomly from a Gaussian prior with zero mean and known covariance $\Sigma$ so that ${p(x)\sim\mathcal{N}(0,\Sigma)}$. 
%The likelihood function can be also  written by marginalizing over the nuisance variables  $\mathcal{L}(y|x)  = \sum_{{\theta_\ell}\in\mathcal{T}} \mathcal{L}(y|\theta_\ell,x)$.
%Hence,  a simple calculation shows that the log-likelihood  then takes the form (up to a constant, cf.~\cite{bendory2017bispectrum,abbe2018multireference})
%\begin{equation}
%\log \mathcal{L}(y|x)  = -\frac{1}{2\sigma^2}\sum_{i=1}^{N}\|y_i - TR_{\theta_i}x\| - \frac{1}{2}x^T\Sigma^{-1}x.
%\end{equation}
%The algorithm aims to compute the MMLE by marginalizing over the the cyclic shifts, that is,  $\log \mathcal{L}(y|x)  = \sum_{\theta\in\mathcal{T}}\log \mathcal{L}(y|\theta,x)$. 
Computing the maximum of the likelihood function directly is intractable
as one needs to evaluate the likelihood for $L^N$ combinations of  rotations. The EM algorithm suggests a framework to  estimate the maximum of the likelihood iteratively. It can be shown that EM converges to local maximum of the likelihood function~\cite{dempster1977maximum}. 

Each EM iteration consists of two steps.
The first, called the E-step, computes the expected value of the likelihood function with respect to the rotation, given the current estimate of the signal $x_t$ and the data $y_1,\ldots,y_N$:
\begin{equation}
\begin{split}
Q(x|x_t) &= \E_{\theta|y_1,\ldots,y_N,x_t}\left\{\ \log \mathcal{L}(y_1,\ldots,y_N,\theta|x_t)  \right\} \\
& = -\frac{1}{2\sigma^2}\sum_{i=1}^{N}\sum_{\theta_\ell\in\mathcal{T}}w_{i,\ell}\|y_i - TR_{\theta_\ell}x\| - \frac{1}{2}x^T\Sigma^{-1}x,
\end{split}
\end{equation} 
where 
\begin{equation} \label{eq:em_weights}
w_{i,\ell} = P[\theta_i = \theta_\ell] = \frac{e^{\frac{-1}{2\sigma^2}\|y_i-TR_{\theta_\ell} x_t\|^2 }}{\sum_{{\theta_\ell}\in\mathcal{T}}e^{\frac{-1}{2\sigma^2}\|y_i-TR_{\theta_\ell} x_t\|^2 }}.
\end{equation}
Then,  the second step is to maximize $Q$ with respect to $x$. 
The solution is obtained by solving the linear system of equations
\begin{equation} \label{eq:em_linear_system}
Ax = b,
\end{equation}
where 
\begin{align}
A :=&  \Sigma^{-1} + \frac{1}{\sigma^2}\sum_{i,\ell}\omega_{i,\ell} (R_{\theta_\ell}^{-1}T^TTR_{\theta_\ell}),\\ 
b :=&   \frac{1}{\sigma^2} \sum_{i,\ell}\omega_{i,\ell}R_\tau^{-1}T^Ty_i.
\end{align}
The EM is then iterate between computing the weights~\eqref{eq:em_weights} and solving the linear system~\eqref{eq:em_linear_system} until convergences. 

In the special case in which the linear operator is the sampling operator~\eqref{eq:model}, computing the weights and constructing $A$ and $b$ reduces to computing a set of correlations that be executed  efficiently using FFT. 
A code for the efficient implementation and scripts for all experiments presented in the next section can be found in TKTK. To avoid local maxima, the code enables multiple initializations. Among the initializations, we choose the estimation that achieve  maximal likelihood, that can be computed according to~\eqref{eq:likelihood}. If the signal is assumed to have a strict bandlimit of $B$ frequencies, then the output of each EM iteration is projected onto its first $B$ frequencies.

\section{Numerical results}

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{XP2}
	\caption{\label{fig:XP2} caption}	
\end{figure}


\begin{figure}[h]
	\centering
	  \includegraphics[scale=1]{XP1}
	  \caption{\label{fig:XP1} caption}	
\end{figure}



\section{Discussion} \label{sec:future_work}


In this work we studied the possibility for super-resolution from multiple signals in a noisy environment using the third-order autocorrelation.
To use higher-order autocorrelations, more measurements should be collected: the number of measurements to scale (asymptotically) like $\sigma^{2q}$ to estimate the $q$th autocorrelation accurately.
If that many measurements are available, the $q$th autocorrelation provides $O(L^{q-1})$ polynomial equations of the sought signals.
Based on our analysis and the reduction of the super-resolution problem to the heterogeneous MRA model~\eqref{eq:heter_mra}, we expect that the $q\geq3$ autocorrelation would identify $O(L^{q-1})$ frequencies. 
Such a  result will follow directly from the generalization of~\cite{bandeira2017estimation} to higher moments. 

The main motivation of this work arises from cryo-EM and XFEL. 
The measurements in these applications  (under simplifying assumption that are beyond the scope of this paper)
agree with the general MRA model~\eqref{eq:model} where $g\in SO(3)$ (the group of 3-D rotations),  the 3-D signal $x$ is assumed to be bounded in a ball (``bandlimited'' volume), and the linear operator $T$ collects samples from the the tomographic projection of $g\cdot x$ from a fixed direction. The question then would be whether the obtainable resolution of a 3-D reconstruction algorithm can overtake the resolution dedicated by the sampling operator---that is, the resolution of the 2-D images. Recent evidences suggest that the answer is positive~\cite{chen2018single}. Extending our analysis to this case is highly non-trivial and we leave it for future research. 
In addition, it would interesting to extend the model~\eqref{eq:model} to incorporate  different distributions of rotations (since the  distribution in cryo-EM is typically non-uniform) and other type of noise statistics (e.g., the  XFEL images are corrupted with Poisson noise).

\bibliographystyle{plain}
\bibliography{ref}


\appendix

\section{Proof of Lemma~\ref{lem:discrete_continuous}}  
\label{sec:proof_lem:discrete_continuous}

In expectation the noise affects the autocorrelations only through bias terms with depends solely on $\sigma^2$. Specifically, these bias terms are equal to zero for the first autocorrelation, $\sigma^2\delta[n]$ for the second autocorrelation, and 
$\sigma^2\hat{x}[0](\delta[n_1,0]+\delta[0,n_2]+\delta[n_1,n_2])$ for the third autocorrelation (see for instance~\cite{bendory2017bispectrum}). This is true for both discrete and continuous models. Hereafter, we continue the proof by neglecting the effects of the noise.

We commence by computing the autocorrelations for the discrete model~\eqref{eq:discrete_model} and them proceed with the continuous model~\eqref{eq:model}.
We start with the expected value of the mean:
\begin{equation}
\begin{split}
\E M_1(y^d) =  \frac{1}{L} \sum_{\ell=0}^{L-1}\E y^d[\ell] =  \frac{1}{LB}\sum_{\tau=0}^{B-1}\sum_{\ell=0}^{L-1}
\sum_{b=-\tB}^{\tB}\hat{x}[b]e^{2\pi\I b \left(\frac{\ell}{L} + \frac{\tau}{B}\right) }.
\end{split}
\end{equation}
Since $\frac{1}{B}\sum_{\tau=0}^{B-1}e^{2\pi\I b \tau/B}=\delta_b$, we conclude that 
\begin{equation} \label{eq:mean}
\begin{split}
\E M_1(y^d) = \hx[0].
\end{split}
\end{equation}

We can now proceed with the second moment:
\begin{equation} \label{eq:ps}
\begin{split}
\E M_2(y^d)[n] &= \frac{1}{L}\sum_{\ell=0}^{L-1} \E y^d[\ell]\overline{y^d}[\ell-n] \\ &=  \frac{1}{LB}\sum_{\tau=0}^{B-1}\sum_{\ell=0}^{L-1}
\left(\sum_{b_1=-\tB}^{\tB}\hat{x}[b_1]e^{2\pi\I b_1 \left(\frac{\ell}{L} + \frac{\tau}{B}\right)} \right)
\left(\sum_{b_2=-\tB}^{\tB}\overline{\hx}[b_2]e^{-2\pi\I b_2 \left(\frac{\ell-n}{L} + \frac{\tau}{B}\right)} \right) \\
&=
\sum_{b=-\tB}^{\tB}\vert \hat{x}[b]\vert ^2e^{2\pi\I bn/L},
\end{split}
\end{equation}
where $\overline{z}$ denotes the conjugate of $z$.
Taking the $L$-points DFT of $\E M_2(y^d)$ results in
\begin{equation}
\begin{split}
\E \hat{M}_2(y^d)[k] &= \sum_{n=0}^{L-1}\E M_2(y^d)[n]e^{-2\pi\I nk/L} = \sum_{b=-\tB}^{\tB}\vert \hat{x}[b]\vert^2\sum_{n=0}^{L-1}e^{-2\pi\I n(k-b)/L}.
\end{split}
\end{equation}
The last term is not zero if and only if $b = k + pL$ for some $p\in\mathbb{Z}$.  Hence, we conclude that the observed power spectrum is a mix of power spectra:
\begin{equation}
\begin{split}
\frac{1}{L}\E \hat{M}_2(y^d)[k] &=  \sum_{-(K-1)}^{K-1} \vert \hat{x}[(q+PL)\bmod L]\vert^2.
\end{split}
\end{equation}

We now proceed with the third-order autocorrelation:
\begin{equation} \label{eq:3rd_moments}
\begin{split}
\E M_3(y^d)[n_1,n_2] &=   \frac{1}{LB}\sum_{\tau=0}^{B-1}\sum_{\ell=0}^{L-1} 
\left(\sum_{b_1=-\tB}^{\tB}\hat{x}[b_1]e^{2\pi\I b_1 \left(\frac{\ell-n_1}{L} + \frac{\tau}{B}\right) }\right) \\
& \times
\left(\sum_{b_2=-\tB}^{\tB}\hat{x}[b_2]e^{2\pi\I b_2 \left(\frac{\ell-n_2}{L} + \frac{\tau}{B}\right) } \right)  \left(\sum_{b_3=-\tB}^{\tB}\hat{x}[b_3]e^{2\pi\I b_3 \left(\frac{\ell}{L} + \frac{\tau}{B}\right) }\right). 
\end{split}
\end{equation}
By changing the order of the summations, and noting that 
\begin{equation} \label{eq:delta}
\frac{1}{B}\sum_{\tau=0}^{B-1}e^{2\pi\I\tau (b_1+b_2+b_3)/B} = \delta_{b_1+b_2+b_3\bmod B},
\end{equation}
we get 
\begin{equation}
\begin{split}
\E M_3(y^d)[n_1,n_2]= 
\sum_{b_1=-\tB}^{\tB}\sum_{b_2=-\tB}^{\tB}\hat{x}[b_1]\hat{x}[b_2]\hat{x}[(-b_1-b_2)\bmod B]e^{-2\pi\I (b_1n_1 + b_2n_2)/L}.
\end{split}
\end{equation}
This expression can be understood in the Fourier domain by taking the $L\times L$ points DFT: 
\begin{equation} \label{eq:mix_bispectra}
\begin{split}
\E \hat{M_3}(y^d)[k_1,k_2] &=  
\sum_{b_1=-\tB}^{\tB}\sum_{b_2=-\tB}^{\tB}\hat{x}[b_1]\hat{x}[b_2]\hat{x}[(-b_1-b_2)\bmod B]\sum_{n_1=0}^{L-1}\sum_{n_2=0}^{L-1}e^{-2\pi\I (n_1(b_1+k_1) + n_2(b_2+k_2))/L} \\ & = L^2\sum_{p_1=-(K-1)}^{K-1} \sum_{p_2=-(K-1)}^{K-1} \hat{x}[(k_1 + p_1L)] \hat{x}[(k_2 + p_2L)]\hat{x}[(-q_1-q_2 - (p_1+p_2)L)\bmod B].
\end{split}
\end{equation}

Next, we compute the autocorrelations for the continuous setup.
The first autocorrelation is given by 
\begin{equation}
\begin{split}
\E M_1(y) = \E\left\{ \frac{1}{L} \sum_{\ell=0}^{L-1} y[\ell]\right\} =
 \int_{0}^1 \frac{1}{L} \sum_{\ell=0}^{L-1} \sum_{b=-\tB}^{\tB}\hat{x}[b]e^{2\pi\I b \left(\frac{\ell}{L} + \theta\right) } d\theta. 
\end{split}
\end{equation}
Since $\int_{0}^{1}e^{2\pi\I\theta b}d\theta=\delta_{b}$, we get 
\begin{equation}
\begin{split}
\E M_1(y) =  \hat{x}[0]. 
\end{split}
\end{equation}

Similarly, the second-order autocorrelation can be computed to be 
\begin{equation}
\begin{split}
\E M_2(y)[n] &= 
\int_{0}^1 \frac{1}{L} \sum_{\ell=0}^{L-1} y[\ell] \overline{y[\ell-n]} d\theta  \\ &=
 \int_{0}^1 \frac{1}{L} \sum_{\ell=0}^{L-1} 
\left(\sum_{b_1=-\tB}^{\tB}\hat{x}[b_1]e^{2\pi\I b _1\left(\frac{\ell}{L} + \theta\right) }\right) \left(\sum_{b_2=-\tB}^{\tB}\hat{x}[b_2]e^{-2\pi\I b_2\left(\frac{\ell-n}{L} + \theta\right) }\right) d\theta. 
\end{split}
\end{equation}
The integration over $\theta$ imposes that the sum is not zero if and only if $b_1=b_2$ and thus we deduce
\begin{equation}
\begin{split}
\E M_2(y)[n]  =
&  \sum_{b=-\tB}^{\tB}|\hat{x}[b]|^2e^{2\pi\I bn/L}. 
\end{split}
\end{equation}

The third-order autocorrelation is given by 
\begin{equation}
\begin{split}
M_3(y)[n_1,n_2] 
&= \int_{0}^{1}\frac{1}{L}\sum_{\ell=0}^{L-1} 
\left(\sum_{b_1=-\tB}^{\tB}\hat{x}[b_1]e^{2\pi\I b_1 \left(\frac{\ell-n_1}{L} + \theta\right) }\right) 
\left(\sum_{b_2=-\tB}^{\tB}\hat{x}[b_2]e^{2\pi\I b_2 \left(\frac{\ell-n_2}{L} + \theta\right) } \right) \\
&\times \left(\sum_{b_3=-\tB}^{\tB}\hat{x}[b_3]e^{2\pi\I b_3 \left(\frac{\ell}{L} + \theta\right) }\right). 
\end{split}
\end{equation}
Since $\int_{0}^{1}e^{2\pi\I\theta(b_1+b_2+b_3)}d\theta=\delta_{b_1+b_2+b_3}$, we get 
\begin{equation}
\begin{split}
M_3(y)[n_1,n_2] =   \sum_{b_1=-\tB}^{\tB}\sum_{b_2=-\tB}^{\tB} \hat{x}[b_1]\hat{x}[b_2]\hat{x}[-b_1-b_2]e^{-2\pi\I (b_1n_1+b_2n_2)}.
\end{split}
\end{equation}

\end{document}

